import pandas as pd
import numpy as np
from datetime import timedelta
from sentence_transformers import SentenceTransformer
import hdbscan
import collections
from sklearn.metrics.pairwise import euclidean_distances
from app.db.connection import check_db_connection
from app.services.sentiment import get_sentiment_score_for_article
from app.services.keyword_extractor import extract_keywords, extract_named_entities, restore_named_entities, kw_model
from app.services.summarize import summarize_top3_articles

def week_start_sunday(input_date_str: str) -> str:
    """
    ì£¼ì–´ì§„ ë‚ ì§œ(input_date_str)ê°€ í¬í•¨ëœ ì£¼ì˜ ì¼ìš”ì¼(weekstart)ì„ YYYY-MM-DD ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    dt = pd.to_datetime(input_date_str)
    days_to_curr_sunday = (dt.weekday() + 1) % 7
    curr_sunday = dt - timedelta(days=days_to_curr_sunday)
    return curr_sunday.strftime("%Y-%m-%d")

## ì‚°ì—… Agent
def get_articles_by_sector(sector: str, weekstart_sunday: str):
    """
    DBì—ì„œ íŠ¹ì • ì‚°ì—… ì„¹í„°ì™€ ì£¼ì°¨ì— í•´ë‹¹í•˜ëŠ” ê¸°ì‚¬ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    conn = check_db_connection()
    if conn is None:
        return []
    
    try:
        cur = conn.cursor()
        query = """
            SELECT article, date, weekstart_sunday, article_title, stock_symbol
            FROM kb_enterprise_dataset 
            WHERE sector = %s AND weekstart_sunday = %s
            ORDER BY date DESC;
        """
        cur.execute(query, (sector, weekstart_sunday))
        rows = cur.fetchall()
        cur.close()
        conn.close()
        return rows
    except Exception as e:
        print(f"Error fetching articles for sector {sector}: {e}")
        if conn:
            conn.close()
        return []

def get_industry_top3_articles(sector: str, end_date: str):
    """
    ì‚°ì—…êµ°ê³¼ ì¢…ë£Œì¼ì„ ë°›ì•„ í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•œ ìƒìœ„ 3ê°œ ëŒ€í‘œ ê¸°ì‚¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    week_sunday = week_start_sunday(end_date)
    
    # DBì—ì„œ í•´ë‹¹ ì„¹í„°ì˜ ê¸°ì‚¬ë“¤ ê°€ì ¸ì˜¤ê¸°
    articles_data = get_articles_by_sector(sector, week_sunday)
    
    if not articles_data:
        print(f"â— {sector} ì„¹í„°ì˜ {week_sunday} ì£¼ì°¨ì— ê¸°ì‚¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {"top3_articles": [], "week": week_sunday}
    
    # ê¸°ì‚¬ ì œëª© ì¶”ì¶œ ë° í´ë¦¬ë‹
    valid_articles = []
    titles = []
    
    for article, date, weekstart, article_title, stock_symbol in articles_data:
        if article_title and article_title.strip():
            valid_articles.append({
                'article': article,
                'date': date,
                'weekstart': weekstart,
                'article_title': article_title,
                'stock_symbol': stock_symbol
            })
            titles.append(article_title.strip())
    
    if len(titles) < 3:
        print(f"â— {sector} ì„¹í„°ì˜ ê¸°ì‚¬ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (í•„ìš”: 3ê°œ, í˜„ì¬: {len(titles)}ê°œ)")
        return {"top3_articles": [], "week": week_sunday}
    
    # 1) ì„ë² ë”©
    model = SentenceTransformer('all-mpnet-base-v2')
    embeddings = model.encode(titles, show_progress_bar=True)
    
    # 2) í´ëŸ¬ìŠ¤í„°ë§
    clusterer = hdbscan.HDBSCAN(min_cluster_size=3)
    labels = clusterer.fit_predict(embeddings)
    
    # 3) ìƒìœ„ 3ê°œ í´ëŸ¬ìŠ¤í„° ì°¾ê¸°
    cnt = collections.Counter(labels[labels >= 0])
    if len(cnt) < 3:
        print(f"â— í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. (í•„ìš”: 3ê°œ, í˜„ì¬: {len(cnt)}ê°œ)")
        print(f"ğŸ“Š ì´ ê¸°ì‚¬ ìˆ˜: {len(valid_articles)}, í´ëŸ¬ìŠ¤í„°ë§ëœ ê¸°ì‚¬ ìˆ˜: {len(labels[labels >= 0])}, ë…¸ì´ì¦ˆ ê¸°ì‚¬ ìˆ˜: {len(labels[labels == -1])}")
        
        # í´ëŸ¬ìŠ¤í„°ê°€ ë¶€ì¡±í•œ ê²½ìš° ê°€ì¥ í° í´ëŸ¬ìŠ¤í„°ë“¤ê³¼ ë…¸ì´ì¦ˆì—ì„œ ì„ íƒ
        available_clusters = [cl[0] for cl in cnt.most_common()]
        noise_indices = np.where(labels == -1)[0]
        
        top3_articles = []
        
        # ê¸°ì¡´ í´ëŸ¬ìŠ¤í„°ì—ì„œ ëŒ€í‘œ ê¸°ì‚¬ ì„ íƒ
        for cluster_id in available_clusters:
            cluster_idxs = np.where(labels == cluster_id)[0]
            cluster_embs = embeddings[cluster_idxs]
            centroid = cluster_embs.mean(axis=0)
            dists = euclidean_distances([centroid], cluster_embs)[0]
            medoid_pos = cluster_idxs[np.argmin(dists)]
            
            article_data = valid_articles[medoid_pos]
            top3_articles.append(article_data)
        
        print(f"ğŸ” í´ëŸ¬ìŠ¤í„°ì—ì„œ ì„ íƒëœ ê¸°ì‚¬ ìˆ˜: {len(top3_articles)}")
        
        # ë¶€ì¡±í•œ ë§Œí¼ ë…¸ì´ì¦ˆì—ì„œ ì¶”ê°€ ì„ íƒ (ë¬´ì‘ìœ„)
        needed = 3 - len(top3_articles)
        if needed > 0 and len(noise_indices) > 0:
            print(f"ğŸ² ë…¸ì´ì¦ˆì—ì„œ {needed}ê°œ ê¸°ì‚¬ ì¶”ê°€ ì„ íƒ (ì‚¬ìš© ê°€ëŠ¥í•œ ë…¸ì´ì¦ˆ: {len(noise_indices)}ê°œ)")
            selected_noise = np.random.choice(noise_indices, min(needed, len(noise_indices)), replace=False)
            for idx in selected_noise:
                article_data = valid_articles[idx]
                top3_articles.append(article_data)
        
        # ì—¬ì „íˆ ë¶€ì¡±í•˜ë©´ ì „ì²´ì—ì„œ ë¬´ì‘ìœ„ ì„ íƒ
        remaining_needed = 3 - len(top3_articles)
        if remaining_needed > 0:
            print(f"âš ï¸ ì—¬ì „íˆ {remaining_needed}ê°œ ê¸°ì‚¬ ë¶€ì¡± - ì „ì²´ì—ì„œ ë¬´ì‘ìœ„ ì„ íƒ")
            while len(top3_articles) < 3 and len(valid_articles) > len(top3_articles):
                remaining_indices = [i for i in range(len(valid_articles)) 
                                   if valid_articles[i] not in top3_articles]
                if remaining_indices:
                    selected_idx = np.random.choice(remaining_indices)
                    top3_articles.append(valid_articles[selected_idx])
                else:
                    print("âŒ ë” ì´ìƒ ì„ íƒí•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    break
        
        print(f"âœ… ìµœì¢… ì„ íƒëœ ê¸°ì‚¬ ìˆ˜: {len(top3_articles)}")
    else:
        top_clusters = [cl[0] for cl in cnt.most_common(3)]
        top3_articles = []
        
        for cluster_id in top_clusters:
            cluster_idxs = np.where(labels == cluster_id)[0]
            cluster_embs = embeddings[cluster_idxs]
            centroid = cluster_embs.mean(axis=0)
            dists = euclidean_distances([centroid], cluster_embs)[0]
            medoid_pos = cluster_idxs[np.argmin(dists)]
            
            article_data = valid_articles[medoid_pos]
            top3_articles.append(article_data)
    
    # 4) ê° ê¸°ì‚¬ì— ëŒ€í•´ ê°ì„±ì ìˆ˜, í‚¤ì›Œë“œ, ìš”ì•½ ì¶”ê°€
    enriched_articles = []
    conn = check_db_connection()
    
    for article_data in top3_articles:
        # ê°ì„±ì ìˆ˜ ê³„ì‚°
        sentiment_score = get_sentiment_score_for_article(article_data['article'], conn)
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        original_ents, lowered_ents = extract_named_entities(article_data['article'])
        keywords = extract_keywords(article_data['article'], kw_model)
        keywords = restore_named_entities(keywords, original_ents, lowered_ents)
        keyword_list = [kw for kw, _ in keywords]
        
        enriched_article = {
            'article': article_data['article'],
            'date': article_data['date'].strftime('%Y-%m-%d') if hasattr(article_data['date'], 'strftime') else str(article_data['date']),
            'weekstart': article_data['weekstart'].strftime('%Y-%m-%d') if hasattr(article_data['weekstart'], 'strftime') else str(article_data['weekstart']),
            'article_title': article_data['article_title'],
            'stock_symbol': article_data['stock_symbol'],
            'score': sentiment_score,
            'keywords': keyword_list
        }
        enriched_articles.append(enriched_article)
    
    if conn:
        conn.close()
    
    # 5) ìš”ì•½ ì¶”ê°€ (ê¸°ì¡´ í•¨ìˆ˜ í™œìš©ì„ ìœ„í•´ í˜•ì‹ ë§ì¶”ê¸°)
    summary_input = []
    for article in enriched_articles:
        summary_input.append({
            'article': article['article'],
            'date': article['date'],
            'weekstart': article['weekstart'],
            'score': article['score'],
            'pos_cnt': 0,  # ì„ì‹œê°’
            'neg_cnt': 0,  # ì„ì‹œê°’
            'article_title': article['article_title']
        })
    
    summarized_articles = summarize_top3_articles(summary_input)
    
    # ìµœì¢… ê²°ê³¼ êµ¬ì„±
    final_articles = []
    for i, article in enumerate(enriched_articles):
        final_article = article.copy()
        if i < len(summarized_articles):
            final_article['summary'] = summarized_articles[i]['summary']
        else:
            final_article['summary'] = "ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        final_articles.append(final_article)
    
    print(f"=== {sector} ì„¹í„° {week_sunday} ì£¼ì°¨ ìƒìœ„ 3ê°œ í´ëŸ¬ìŠ¤í„° ëŒ€í‘œ ê¸°ì‚¬ ===")
    for i, article in enumerate(final_articles, 1):
        print(f"[ê¸°ì‚¬ {i}] ì¢…ëª©: {article['stock_symbol']}, ì œëª©: {article['article_title']}")
        print(f"ê°ì„±ì ìˆ˜: {article['score']}, í‚¤ì›Œë“œ: {article['keywords'][:3]}")
    
    return {
        "top3_articles": final_articles,
        "week": week_sunday
    }

## ì¦ì‹œ Agent
def get_hot_articles_by_date(start_date: str):
    """
    DBì—ì„œ íŠ¹ì • ì£¼ì°¨ì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  ê¸°ì‚¬ë“¤ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. (ì„¹í„° êµ¬ë¶„ ì—†ìŒ)
    """
    conn = check_db_connection()
    if conn is None:
        return []
    
    try:
        cur = conn.cursor()
        query = """
            SELECT article, date, weekstart_sunday, article_title, stock_symbol, sector
            FROM kb_enterprise_dataset 
            WHERE weekstart_sunday = %s
            ORDER BY date DESC;
        """
        cur.execute(query, (start_date,))
        rows = cur.fetchall()
        cur.close()
        conn.close()
        return rows
    except Exception as e:
        print(f"Error fetching hot articles for date {start_date}: {e}")
        if conn:
            conn.close()
        return []

def get_market_hot_articles(end_date: str):
    week_sunday = week_start_sunday(end_date)
    articles_data = get_hot_articles_by_date(week_sunday)
    if not articles_data:
        print(f"â— {week_sunday} ì£¼ì°¨ì— ê¸°ì‚¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {"top3_articles": [], "week": week_sunday}

    # ê¸°ì‚¬ ì œëª© ì¶”ì¶œ ë° í´ë¦¬ë‹
    valid_articles = []
    titles = []
    for article, date, weekstart, article_title, stock_symbol, sector in articles_data:
        if article_title and article_title.strip():
            valid_articles.append({
                'article': article,
                'date': date,
                'weekstart': weekstart,
                'article_title': article_title,
                'stock_symbol': stock_symbol,
                'sector': sector
            })
            titles.append(article_title.strip())
    if len(titles) < 3:
        print(f"â— {week_sunday} ì£¼ì°¨ì˜ ê¸°ì‚¬ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (í•„ìš”: 3ê°œ, í˜„ì¬: {len(titles)}ê°œ)")
        return {"top3_articles": [], "week": week_sunday}

    # 1) ì„ë² ë”© (í•œ ë²ˆë§Œ)
    model = SentenceTransformer('all-mpnet-base-v2')
    embeddings = model.encode(titles, show_progress_bar=True)

    # 2) í´ëŸ¬ìŠ¤í„°ë§ (í•œ ë²ˆë§Œ)
    clusterer = hdbscan.HDBSCAN(min_cluster_size=3)
    labels = clusterer.fit_predict(embeddings)

    # 3) ìƒìœ„ 3ê°œ í´ëŸ¬ìŠ¤í„° ì°¾ê¸° (í•œ ë²ˆë§Œ)
    cnt = collections.Counter(labels[labels >= 0])
    top3_articles = []
    if len(cnt) < 3:
        print(f"â— í´ëŸ¬ìŠ¤í„° ê°œìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. (í•„ìš”: 3ê°œ, í˜„ì¬: {len(cnt)}ê°œ)")
        print(f"ğŸ“Š ì´ ê¸°ì‚¬ ìˆ˜: {len(valid_articles)}, í´ëŸ¬ìŠ¤í„°ë§ëœ ê¸°ì‚¬ ìˆ˜: {len(labels[labels >= 0])}, ë…¸ì´ì¦ˆ ê¸°ì‚¬ ìˆ˜: {len(labels[labels == -1])}")
        
        # í´ëŸ¬ìŠ¤í„°ê°€ ë¶€ì¡±í•œ ê²½ìš° ê°€ì¥ í° í´ëŸ¬ìŠ¤í„°ë“¤ê³¼ ë…¸ì´ì¦ˆì—ì„œ ì„ íƒ
        available_clusters = [cl[0] for cl in cnt.most_common()]
        noise_indices = np.where(labels == -1)[0]
        # í´ëŸ¬ìŠ¤í„° ëŒ€í‘œ ê¸°ì‚¬ ì„ íƒ
        for cluster_id in available_clusters:
            cluster_idxs = np.where(labels == cluster_id)[0]
            cluster_embs = embeddings[cluster_idxs]
            centroid = cluster_embs.mean(axis=0)
            dists = euclidean_distances([centroid], cluster_embs)[0]
            medoid_pos = cluster_idxs[np.argmin(dists)]
            article_data = valid_articles[medoid_pos]
            top3_articles.append(article_data)
        
        print(f"ğŸ” í´ëŸ¬ìŠ¤í„°ì—ì„œ ì„ íƒëœ ê¸°ì‚¬ ìˆ˜: {len(top3_articles)}")
        
        # ë¶€ì¡±í•œ ë§Œí¼ ë…¸ì´ì¦ˆì—ì„œ ì¶”ê°€ ì„ íƒ (ë¬´ì‘ìœ„)
        needed = 3 - len(top3_articles)
        if needed > 0 and len(noise_indices) > 0:
            print(f"ğŸ² ë…¸ì´ì¦ˆì—ì„œ {needed}ê°œ ê¸°ì‚¬ ì¶”ê°€ ì„ íƒ (ì‚¬ìš© ê°€ëŠ¥í•œ ë…¸ì´ì¦ˆ: {len(noise_indices)}ê°œ)")
            selected_noise = np.random.choice(noise_indices, min(needed, len(noise_indices)), replace=False)
            for idx in selected_noise:
                article_data = valid_articles[idx]
                top3_articles.append(article_data)
        # ì—¬ì „íˆ ë¶€ì¡±í•˜ë©´ ì „ì²´ì—ì„œ ë¬´ì‘ìœ„ ì„ íƒ
        remaining_needed = 3 - len(top3_articles)
        if remaining_needed > 0:
            print(f"âš ï¸ ì—¬ì „íˆ {remaining_needed}ê°œ ê¸°ì‚¬ ë¶€ì¡± - ì „ì²´ì—ì„œ ë¬´ì‘ìœ„ ì„ íƒ")
            while len(top3_articles) < 3 and len(valid_articles) > len(top3_articles):
                remaining_indices = [i for i in range(len(valid_articles)) if valid_articles[i] not in top3_articles]
                if remaining_indices:
                    selected_idx = np.random.choice(remaining_indices)
                    top3_articles.append(valid_articles[selected_idx])
                else:
                    break
        
        print(f"âœ… ìµœì¢… ì„ íƒëœ ê¸°ì‚¬ ìˆ˜: {len(top3_articles)}")
    else:
        top_clusters = [cl[0] for cl in cnt.most_common(3)]
        for cluster_id in top_clusters:
            cluster_idxs = np.where(labels == cluster_id)[0]
            cluster_embs = embeddings[cluster_idxs]
            centroid = cluster_embs.mean(axis=0)
            dists = euclidean_distances([centroid], cluster_embs)[0]
            medoid_pos = cluster_idxs[np.argmin(dists)]
            article_data = valid_articles[medoid_pos]
            top3_articles.append(article_data)

    # 4) ê° ê¸°ì‚¬ì— ëŒ€í•´ ê°ì„±ì ìˆ˜, í‚¤ì›Œë“œ, ìš”ì•½ ì¶”ê°€ (í•œ ë²ˆë§Œ)
    conn = check_db_connection()
    enriched_articles = []
    for article_data in top3_articles:
        # ê°ì„±ì ìˆ˜, í‚¤ì›Œë“œ ì¶”ì¶œ í•œ ë²ˆë§Œ
        sentiment_score = get_sentiment_score_for_article(article_data['article'], conn)
        original_ents, lowered_ents = extract_named_entities(article_data['article'])
        keywords = extract_keywords(article_data['article'], kw_model)
        keywords = restore_named_entities(keywords, original_ents, lowered_ents)
        keyword_list = [kw for kw, _ in keywords]
        enriched_article = {
            'article': article_data['article'],
            'date': article_data['date'].strftime('%Y-%m-%d') if hasattr(article_data['date'], 'strftime') else str(article_data['date']),
            'weekstart': article_data['weekstart'].strftime('%Y-%m-%d') if hasattr(article_data['weekstart'], 'strftime') else str(article_data['weekstart']),
            'article_title': article_data['article_title'],
            'stock_symbol': article_data['stock_symbol'],
            'sector': article_data['sector'],
            'score': sentiment_score,
            'keywords': keyword_list
        }
        enriched_articles.append(enriched_article)
    if conn:
        conn.close()

    # 5) ìš”ì•½ ìƒì„± (í•œ ë²ˆë§Œ)
    summary_input = [{
        'article': article['article'],
        'date': article['date'],
        'weekstart': article['weekstart'],
        'score': article['score'],
        'pos_cnt': 0,
        'neg_cnt': 0,
        'article_title': article['article_title']
    } for article in enriched_articles]
    summarized_articles = summarize_top3_articles(summary_input)

    # ìµœì¢… ê²°ê³¼ êµ¬ì„±
    final_articles = []
    for i, article in enumerate(enriched_articles):
        final_article = article.copy()
        if i < len(summarized_articles):
            final_article['summary'] = summarized_articles[i]['summary']
        else:
            final_article['summary'] = "ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        final_articles.append(final_article)

    print(f"=== {week_sunday} ì£¼ì°¨ ì‹œì¥ í•«í•œ ê¸°ì‚¬ TOP 3 ===")
    for i, article in enumerate(final_articles, 1):
        print(f"[ê¸°ì‚¬ {i}] ì„¹í„°: {article['sector']}, ì¢…ëª©: {article['stock_symbol']}, ì œëª©: {article['article_title']}")
        print(f"ê°ì„±ì ìˆ˜: {article['score']}, í‚¤ì›Œë“œ: {article['keywords'][:3]}")

    return {
        "top3_articles": final_articles,
        "week": week_sunday
    }
    
    # 5) ìš”ì•½ ì¶”ê°€ (ê¸°ì¡´ í•¨ìˆ˜ í™œìš©ì„ ìœ„í•´ í˜•ì‹ ë§ì¶”ê¸°)
    summary_input = []
    for article in enriched_articles:
        summary_input.append({
            'article': article['article'],
            'date': article['date'],
            'weekstart': article['weekstart'],
            'score': article['score'],
            'pos_cnt': 0,  # ì„ì‹œê°’
            'neg_cnt': 0,  # ì„ì‹œê°’
            'article_title': article['article_title']
        })
    
    summarized_articles = summarize_top3_articles(summary_input)
    
    # ìµœì¢… ê²°ê³¼ êµ¬ì„±
    final_articles = []
    for i, article in enumerate(enriched_articles):
        final_article = article.copy()
        if i < len(summarized_articles):
            final_article['summary'] = summarized_articles[i]['summary']
        else:
            final_article['summary'] = "ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        final_articles.append(final_article)
    
    print(f"=== {week_sunday} ì£¼ì°¨ ì‹œì¥ í•«í•œ ê¸°ì‚¬ TOP 3 ===")
    for i, article in enumerate(final_articles, 1):
        print(f"[ê¸°ì‚¬ {i}] ì„¹í„°: {article['sector']}, ì¢…ëª©: {article['stock_symbol']}, ì œëª©: {article['article_title']}")
        print(f"ê°ì„±ì ìˆ˜: {article['score']}, í‚¤ì›Œë“œ: {article['keywords'][:3]}")
    
    return {
        "top3_articles": final_articles,
        "week": week_sunday
    }
